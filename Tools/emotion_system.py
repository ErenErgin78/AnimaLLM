"""
Duygu Analizi Sistemi - LoRA Model Entegrasyonu
===============================================

Bu modÃ¼l LoRA model ile metin Ã¼retimi ve LLM ile duygu analizi yapar.
- LoRA modelinden kullanÄ±cÄ± mesajÄ±na cevap Ã¼retir
- LLM'den (Gemini/GPT) duygu analizi yapar
- mood_emojis.json'dan duyguya gÃ¶re emoji seÃ§er
"""

import os
import json
import random
import re
import html
from typing import Any, Dict, Optional
from datetime import datetime
from pathlib import Path
from openai import OpenAI

# LoRA model iÃ§in gerekli importlar
try:
    from transformers import GPT2LMHeadModel, AutoTokenizer
    from peft import PeftModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("[WARNING] transformers veya peft kÃ¼tÃ¼phaneleri bulunamadÄ±. LoRA model kullanÄ±lamayacak.")

# GÃ¼venlik sabitleri
MAX_EMOTION_MESSAGE_LENGTH = 1000
DANGEROUS_EMOTION_PATTERNS = [
    r'<script[^>]*>.*?</script>',
    r'javascript:',
    r'data:text/html',
    r'vbscript:',
    r'on\w+\s*=',
    r'<iframe[^>]*>',
    r'<object[^>]*>',
    r'<embed[^>]*>',
]

# Duygu â†’ emoji veri kaynaÄŸÄ±nÄ± yÃ¼kle (uygulama baÅŸÄ±nda bir kez)
MOOD_EMOJIS: Dict[str, list[str]] = {}
# KalÄ±cÄ± depolama dosyalarÄ± (proje kÃ¶kÃ¼ /data)
DATA_DIR = Path(__file__).resolve().parents[1] / "data"
CHAT_HISTORY_FILE = DATA_DIR / "chat_history.txt"
MOOD_COUNTER_FILE = DATA_DIR / "mood_counter.txt"

try:
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    data_path = DATA_DIR / "mood_emojis.json"
    if data_path.exists():
        MOOD_EMOJIS = json.loads(data_path.read_text(encoding="utf-8"))
    # DosyalarÄ± oluÅŸtur
    if not CHAT_HISTORY_FILE.exists():
        CHAT_HISTORY_FILE.write_text("", encoding="utf-8")
    if not MOOD_COUNTER_FILE.exists():
        # Yeni format: boÅŸ JSON array (zaman damgalÄ± kayÄ±tlar)
        MOOD_COUNTER_FILE.write_text("[]", encoding="utf-8")
except Exception:
    MOOD_EMOJIS = {}


class EmotionChatbot:
    def __init__(self, client: OpenAI = None) -> None:
        """Emotion chatbot baÅŸlatÄ±r - LoRA model ve LLM (Gemini/GPT) hazÄ±rlar"""
        self.client = client
        self.use_gemini = False
        if client is None:
            self.use_gemini = True
            # Gemini API'yi yapÄ±landÄ±r - uyarÄ±larÄ± bastÄ±r
            import google.generativeai as genai
            import warnings
            import logging
            warnings.filterwarnings("ignore", category=UserWarning)
            os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
            os.environ['GRPC_VERBOSITY'] = 'ERROR'
            logging.getLogger("google").setLevel(logging.ERROR)
            logging.getLogger("google.api_core").setLevel(logging.ERROR)
            logging.getLogger("absl").setLevel(logging.ERROR)
            genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        
        self.stats: Dict[str, Any] = {
            "requests": 0,
            "last_request_at": None,
        }
        self.allowed_moods = [
            "Mutlu", "ÃœzgÃ¼n", "Ã–fkeli", "ÅžaÅŸkÄ±n", "UtanmÄ±ÅŸ",
            "EndiÅŸeli", "GÃ¼lÃ¼mseyen", "FlÃ¶rtÃ¶z", "SorgulayÄ±cÄ±", "Yorgun"
        ]
        self.emotion_counts: Dict[str, int] = {m: 0 for m in self.allowed_moods}
        
        # KalÄ±cÄ± sayaÃ§larÄ± yÃ¼kle
        persisted = self._load_mood_counts()
        if persisted:
            for k, v in persisted.items():
                if k in self.emotion_counts and isinstance(v, int):
                    self.emotion_counts[k] = v
        
        # LoRA model ve tokenizer iÃ§in lazy loading
        self.lora_model = None
        self.lora_tokenizer = None
        self._lora_loaded = False
        self._lora_loading = False  # Asenkron yÃ¼kleme durumu

    def _sanitize_emotion_input(self, text: str) -> str:
        """Duygu sistemi iÃ§in gÃ¼venli input sanitization"""
        if not text:
            return ""
        text = html.escape(text, quote=True)
        for pattern in DANGEROUS_EMOTION_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                print(f"[SECURITY] Duygu sisteminde tehlikeli pattern: {pattern}")
                return "[GÃ¼venlik nedeniyle mesaj filtrelendi]"
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def _validate_emotion_message_length(self, text: str) -> bool:
        """Duygu mesajÄ± uzunluk kontrolÃ¼"""
        return len(text) <= MAX_EMOTION_MESSAGE_LENGTH

    def _convert_messages_to_prompt(self, messages: list[Dict[str, Any]]) -> str:
        """OpenAI mesaj formatÄ±nÄ± Gemini prompt formatÄ±na Ã§evirir"""
        prompt_parts = []
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if content:
                if role == "system":
                    prompt_parts.append(f"Sistem: {content}")
                elif role == "user":
                    prompt_parts.append(f"KullanÄ±cÄ±: {content}")
                elif role == "assistant":
                    prompt_parts.append(f"Asistan: {content}")
        return "\n".join(prompt_parts)

    def _load_mood_counts(self) -> Dict[str, int]:
        """KalÄ±cÄ± duygu sayaÃ§larÄ±nÄ± yÃ¼kler - eski ve yeni formatÄ± destekler"""
        try:
            raw = MOOD_COUNTER_FILE.read_text(encoding="utf-8").strip()
            if not raw:
                return {}
            
            data = json.loads(raw)
            
            # Yeni format: JSON array (zaman damgalÄ± kayÄ±tlar)
            if isinstance(data, list):
                counts: Dict[str, int] = {m: 0 for m in self.allowed_moods}
                for record in data:
                    if isinstance(record, dict):
                        mood = str(record.get("mood", "")).strip()
                        if mood in counts:
                            counts[mood] += 1
                return counts
            
            # Eski format: JSON object (sayÄ±lar)
            elif isinstance(data, dict):
                return {str(k): int(v) for k, v in data.items()}
        except Exception:
            pass
        return {}

    def _load_mood_history(self) -> list[Dict[str, Any]]:
        """Zaman damgalÄ± duygu kayÄ±tlarÄ±nÄ± yÃ¼kler"""
        try:
            raw = MOOD_COUNTER_FILE.read_text(encoding="utf-8").strip()
            if not raw:
                return []
            
            data = json.loads(raw)
            
            # Yeni format: JSON array
            if isinstance(data, list):
                return data
            
            # Eski format: JSON object - yeni formata dÃ¶nÃ¼ÅŸtÃ¼r
            elif isinstance(data, dict):
                history = []
                today = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                for mood, count in data.items():
                    if isinstance(count, int) and count > 0:
                        # Her sayÄ± iÃ§in bir kayÄ±t oluÅŸtur (bugÃ¼nÃ¼n tarihiyle)
                        for _ in range(min(count, 1000)):  # GÃ¼venlik iÃ§in maksimum 1000
                            history.append({
                                "mood": str(mood).strip(),
                                "timestamp": today
                            })
                return history
        except Exception as e:
            print(f"[EMOTION] Duygu geÃ§miÅŸi yÃ¼kleme hatasÄ±: {e}")
        return []

    def _append_mood_record(self, mood: str) -> None:
        """Duygu kaydÄ±nÄ± zaman damgasÄ±yla kalÄ±cÄ± olarak ekler"""
        try:
            # Mevcut kayÄ±tlarÄ± yÃ¼kle
            history = self._load_mood_history()
            
            # Yeni kayÄ±t ekle
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            history.append({
                "mood": str(mood).strip(),
                "timestamp": timestamp
            })
            
            # GÃ¼venlik: Maksimum 10000 kayÄ±t tut (eski kayÄ±tlarÄ± koru)
            if len(history) > 10000:
                # En eski kayÄ±tlarÄ± sil, son 10000'i tut
                history = history[-10000:]
            
            # Dosyaya kaydet
            MOOD_COUNTER_FILE.write_text(
                json.dumps(history, ensure_ascii=False, indent=2),
                encoding="utf-8",
            )
            print(f"[EMOTION] Duygu kaydÄ± dosyaya yazÄ±ldÄ±: {mood} ({timestamp})")
        except Exception as e:
            print(f"[EMOTION] Duygu kaydÄ± ekleme hatasÄ±: {e}")

    def _save_mood_counts(self) -> None:
        """Geriye uyumluluk iÃ§in - artÄ±k kullanÄ±lmÄ±yor, _append_mood_record kullanÄ±lmalÄ±"""
        # Bu metod artÄ±k kullanÄ±lmÄ±yor ama geriye uyumluluk iÃ§in bÄ±rakÄ±ldÄ±
        pass

    def _append_chat_history(self, user_message: str, response_text: str) -> None:
        """KonuÅŸma geÃ§miÅŸini dosyaya ekler"""
        try:
            line = json.dumps({
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "user": user_message,
                "response": response_text
            }, ensure_ascii=False)
            with CHAT_HISTORY_FILE.open("a", encoding="utf-8") as f:
                f.write(line + "\n")
        except Exception:
            pass

    def get_functions(self) -> list[Dict[str, Any]]:
        """Emotion sistemi iÃ§in function-calling kullanÄ±lmÄ±yor"""
        return []

    def preload_lora_model_async(self) -> None:
        """Asenkron olarak LoRA modelini Ã¶nceden yÃ¼kle (program baÅŸlatÄ±ldÄ±ÄŸÄ±nda)"""
        if self._lora_loading or self._lora_loaded:
            return
        
        self._lora_loading = True
        print("[EMOTION] Asenkron LoRA model yÃ¼kleme baÅŸlatÄ±lÄ±yor...")
        
        import threading
        def load_in_background():
            try:
                self._load_lora_model()
                print("[EMOTION] LoRA model asenkron olarak yÃ¼klendi âœ…")
            except Exception as e:
                print(f"[EMOTION] LoRA model yÃ¼kleme hatasÄ±: {e}")
                import traceback
                print(f"[EMOTION] Traceback: {traceback.format_exc()}")
            finally:
                self._lora_loading = False
        
        thread = threading.Thread(target=load_in_background, daemon=True)
        thread.start()
    
    def _load_lora_model(self):
        """LoRA modeli yÃ¼kler - Base model Ã¼zerine adaptÃ¶r takÄ±lÄ±r"""
        if self._lora_loaded:
            return
        
        if not TRANSFORMERS_AVAILABLE:
            print("[ERROR] LoRA model yÃ¼klenemedi: transformers/peft kÃ¼tÃ¼phaneleri bulunamadÄ±")
            self._lora_loaded = True
            return
        
        try:
            import torch
            
            # LoRA adaptÃ¶r yolunu dinamik olarak belirle
            # Tools klasÃ¶rÃ¼nden bir seviye yukarÄ± proje kÃ¶kÃ¼ne Ã§Ä±k ve Lora/Model/main'i hedefle
            project_root = Path(__file__).resolve().parents[1]
            base_path = project_root
            lora_path = project_root / "Lora" / "Model" / "main"
            
            if not lora_path.exists():
                print(f"[ERROR] LoRA adaptÃ¶r yolu bulunamadÄ±: {lora_path}")
                self._lora_loaded = True
                return
            
            # Adapter dosyalarÄ±nÄ± dinamik olarak kontrol et
            # adapter_config.json ve lora.safetensors (veya adapter_model.safetensors) dosyalarÄ±nÄ± ara
            adapter_config_path = lora_path / "adapter_config.json"
            
            # Model dosyasÄ±nÄ± kontrol et - Ã¶nce adapter_model.safetensors, sonra lora.safetensors
            adapter_model_path = lora_path / "adapter_model.safetensors"
            if not adapter_model_path.exists():
                adapter_model_path = lora_path / "lora.safetensors"
            
            # EÄŸer adapter_config.json yoksa, "en iyi" klasÃ¶rÃ¼nden al (fallback)
            if not adapter_config_path.exists():
                fallback_config_path = project_root / "Lora" / "Model" / "en iyi" / "adapter_config.json"
                if fallback_config_path.exists():
                    print(f"[LoRA] adapter_config.json main klasÃ¶rÃ¼nde bulunamadÄ±, 'en iyi' klasÃ¶rÃ¼nden kullanÄ±lÄ±yor")
                    # Config'i main klasÃ¶rÃ¼ne kopyala veya doÄŸrudan kullan
                    # PeftModel.from_pretrained zaten config dosyasÄ±nÄ± kendi bulabilir
                    # Ama en gÃ¼venlisi main klasÃ¶rÃ¼ne kopyalamak
                    import shutil
                    try:
                        shutil.copy2(fallback_config_path, adapter_config_path)
                        print(f"[LoRA] adapter_config.json 'en iyi' klasÃ¶rÃ¼nden main klasÃ¶rÃ¼ne kopyalandÄ±")
                    except Exception as e:
                        print(f"[WARNING] adapter_config.json kopyalanamadÄ±: {e}")
                        # Fallback olarak "en iyi" klasÃ¶rÃ¼nÃ¼ kullan
                        lora_path = project_root / "Lora" / "Model" / "en iyi"
                        adapter_config_path = lora_path / "adapter_config.json"
                        adapter_model_path = lora_path / "adapter_model.safetensors"
            
            if not adapter_config_path.exists():
                print(f"[ERROR] adapter_config.json bulunamadÄ±: {lora_path}")
                self._lora_loaded = True
                return
            
            if not adapter_model_path.exists():
                print(f"[ERROR] LoRA model dosyasÄ± bulunamadÄ± (adapter_model.safetensors veya lora.safetensors): {lora_path}")
                self._lora_loaded = True
                return
            
            print(f"[LoRA] Base model yÃ¼kleniyor: ytu-ce-cosmos/turkish-gpt2-medium")
            print(f"[LoRA] AdaptÃ¶r yolu: {lora_path}")
            
            # Base model ve tokenizer'Ä± yÃ¼kle
            base_model_name = "ytu-ce-cosmos/turkish-gpt2-medium"
            use_gpu = torch.cuda.is_available()
            
            base_model = GPT2LMHeadModel.from_pretrained(
                base_model_name,
                torch_dtype=torch.float16 if use_gpu else torch.float32
            )
            
            if use_gpu:
                base_model = base_model.cuda()
                print(f"[LoRA] Base model GPU'ya taÅŸÄ±ndÄ±")
            
            # Tokenizer'Ä± yÃ¼kle - Ã¶nce local'den dene, yoksa base model'den
            tokenizer_path = lora_path / "tokenizer.json"
            if tokenizer_path.exists():
                print(f"[LoRA] Local tokenizer bulundu, yÃ¼kleniyor...")
                tokenizer = AutoTokenizer.from_pretrained(str(lora_path))
            else:
                tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            
            # Pad token ekle (eÄŸer yoksa)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token if hasattr(tokenizer, 'eos_token') and tokenizer.eos_token else tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            
            print("[LoRA] Base model yÃ¼klendi, LoRA adaptÃ¶rÃ¼ ekleniyor...")
            
            # LoRA adaptÃ¶rÃ¼nÃ¼ base model Ã¼zerine tak
            # Windows path sorununu Ã§Ã¶zmek iÃ§in: local path iÃ§in from_pretrained yerine load_adapter kullan
            try:
                # Ã–nce from_pretrained ile dene
                lora_path_str = str(lora_path.resolve())
                self.lora_model = PeftModel.from_pretrained(
                    base_model,
                    lora_path_str,
                    device_map="auto" if use_gpu else None
                )
            except Exception as e:
                # EÄŸer path validation hatasÄ± varsa, manuel yÃ¼kleme yap
                error_str = str(e).lower()
                if "repo id" in error_str or "hfvalidationerror" in error_str or "validation" in error_str:
                    print(f"[LoRA] Path validation hatasÄ±, manuel yÃ¼kleme deneniyor...")
                    try:
                        from peft import PeftConfig
                        # Config dosyasÄ±nÄ± oku
                        import json
                        with open(adapter_config_path, 'r', encoding='utf-8') as f:
                            config_dict = json.load(f)
                        
                        # PeftModel oluÅŸtur (base model + config)
                        config = PeftConfig.from_dict(config_dict)
                        self.lora_model = PeftModel(base_model, config, adapter_name="default")
                        
                        # Weight dosyasÄ±nÄ± yÃ¼kle (adapter_model.safetensors veya lora.safetensors)
                        if adapter_model_path.exists():
                            print(f"[LoRA] Weight dosyasÄ± yÃ¼kleniyor: {adapter_model_path.name}")
                            if str(adapter_model_path).endswith('.safetensors'):
                                try:
                                    from safetensors.torch import load_file
                                    state_dict = load_file(str(adapter_model_path))
                                    print(f"[LoRA] Safetensors dosyasÄ± baÅŸarÄ±yla yÃ¼klendi")
                                except ImportError:
                                    print(f"[LoRA WARNING] safetensors.torch bulunamadÄ±, torch ile deneniyor...")
                                    import torch
                                    # .safetensors dosyasÄ±nÄ± torch.load ile aÃ§maya Ã§alÄ±ÅŸma, hata verir
                                    raise ImportError("safetensors kÃ¼tÃ¼phanesi gerekli (.safetensors dosyasÄ± iÃ§in)")
                            else:
                                import torch
                                state_dict = torch.load(str(adapter_model_path), map_location='cpu')
                            
                            # PEFT'in beklediÄŸi format: adapter_model.safetensors zaten doÄŸru formatta olmalÄ±
                            # EÄŸer key'ler base_model.model. ile baÅŸlÄ±yorsa olduÄŸu gibi bÄ±rak
                            # EÄŸer lora_ ile baÅŸlÄ±yorsa default. prefix'i ekle
                            peft_state_dict = {}
                            for key, value in state_dict.items():
                                if key.startswith('base_model.model.'):
                                    # Base model key'leri olduÄŸu gibi bÄ±rak
                                    peft_state_dict[key] = value
                                elif 'lora_' in key or 'default.' in key:
                                    # LoRA key'leri - zaten doÄŸru formatta olabilir
                                    if key.startswith('default.'):
                                        peft_state_dict[key] = value
                                    else:
                                        # default. prefix'i ekle
                                        peft_state_dict[f'default.{key}'] = value
                                else:
                                    # DiÄŸer key'leri de ekle
                                    peft_state_dict[key] = value
                            
                            # State dict'i yÃ¼kle
                            print(f"[LoRA] State dict yÃ¼kleniyor ({len(peft_state_dict)} key)...")
                            missing_keys, unexpected_keys = self.lora_model.load_state_dict(peft_state_dict, strict=False)
                            if missing_keys:
                                print(f"[LoRA WARNING] Eksik keys: {len(missing_keys)} adet (ilk 5: {missing_keys[:5]})")
                            if unexpected_keys:
                                print(f"[LoRA WARNING] Beklenmeyen keys: {len(unexpected_keys)} adet (ilk 5: {unexpected_keys[:5]})")
                            print("[LoRA] Adapter manuel yÃ¼kleme ile baÅŸarÄ±yla yÃ¼klendi")
                        else:
                            raise Exception(f"Adapter weight dosyasÄ± bulunamadÄ±: {adapter_model_path}")
                    except Exception as e2:
                        print(f"[ERROR] Manuel yÃ¼kleme de baÅŸarÄ±sÄ±z oldu: {e2}")
                        import traceback
                        print(f"[ERROR] Traceback: {traceback.format_exc()}")
                        raise e
                else:
                    # DiÄŸer hatalar iÃ§in original hatayÄ± fÄ±rlat
                    raise e
            
            self.lora_tokenizer = tokenizer
            
            # Model tipini ve LoRA durumunu kontrol et
            if hasattr(self.lora_model, 'peft_config'):
                print(f"[LoRA] LoRA adaptÃ¶rÃ¼ baÅŸarÄ±yla eklendi")
                print(f"[LoRA] PEFT config: {list(self.lora_model.peft_config.keys())}")
            else:
                print("[WARNING] LoRA adaptÃ¶rÃ¼ eklenmiÅŸ gibi gÃ¶rÃ¼nmÃ¼yor, PEFT config bulunamadÄ±")
            
            self.lora_model.eval()  # Inference modu
            self._lora_loaded = True
            
            # Model bilgilerini yazdÄ±r
            if use_gpu:
                print(f"[LoRA] Model device: {next(self.lora_model.parameters()).device}")
            
            print("[LoRA] Model baÅŸarÄ±yla yÃ¼klendi ve hazÄ±r")
            
        except Exception as e:
            print(f"[ERROR] LoRA model yÃ¼kleme hatasÄ±: {e}")
            import traceback
            print(f"[ERROR] Traceback: {traceback.format_exc()}")
            self._lora_loaded = True
    
    def _generate_with_lora(self, prompt: str, max_new_tokens: int = 40) -> str:
        """LoRA modelinden metin Ã¼retir - sadece kullanÄ±cÄ± mesajÄ±nÄ± kullanÄ±r"""
        if not TRANSFORMERS_AVAILABLE or self.lora_model is None or self.lora_tokenizer is None:
            return ""
        
        try:
            import torch
            import re
            
            # Prompt'u tokenize et
            model_max_len = getattr(self.lora_tokenizer, 'model_max_length', 512)
            safe_max_length = min(512, model_max_len) if model_max_len < 10000 else 512
            
            encoded = self.lora_tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=safe_max_length
            )
            input_ids = encoded.input_ids
            attention_mask = encoded.attention_mask
            input_length = input_ids.shape[-1]
            
            # GPU'ya taÅŸÄ± (varsa)
            if torch.cuda.is_available():
                input_ids = input_ids.cuda()
                attention_mask = attention_mask.cuda()
            else:
                device = None
                if hasattr(self.lora_model, 'device'):
                    device = self.lora_model.device
                elif hasattr(self.lora_model, 'base_model'):
                    if hasattr(self.lora_model.base_model, 'device'):
                        device = self.lora_model.base_model.device
                
                if device is not None:
                    input_ids = input_ids.to(device)
                    attention_mask = attention_mask.to(device)
            
            # Token ID'lerini gÃ¼venli ÅŸekilde belirle
            if self.lora_tokenizer.pad_token_id is not None:
                pad_token_id = int(self.lora_tokenizer.pad_token_id)
            elif self.lora_tokenizer.eos_token_id is not None:
                pad_token_id = int(self.lora_tokenizer.eos_token_id)
            else:
                pad_token_id = 0
            
            if self.lora_tokenizer.eos_token_id is not None:
                eos_token_id = int(self.lora_tokenizer.eos_token_id)
            elif self.lora_tokenizer.pad_token_id is not None:
                eos_token_id = int(self.lora_tokenizer.pad_token_id)
            else:
                eos_token_id = pad_token_id
            
            # Text generation parametreleri
            min_length_value = input_length + 3
            max_possible_length = safe_max_length if safe_max_length < 10000 else 512
            min_length_value = min(min_length_value, max_possible_length)
            
            with torch.no_grad():
                outputs = self.lora_model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=int(max_new_tokens),
                    min_length=int(min_length_value),
                    do_sample=True,
                    repetition_penalty=float(1.2),
                    no_repeat_ngram_size=int(0),
                    top_k=int(50),
                    top_p=float(0.95),
                    temperature=float(0.8),
                    pad_token_id=int(pad_token_id),
                    eos_token_id=int(eos_token_id)
                )
            
            # Sadece modelin Ã¼rettiÄŸi yanÄ±tÄ± al (prompt'u Ã§Ä±kar)
            response_ids = outputs[0][input_ids.shape[-1]:]
            generated_response = self.lora_tokenizer.decode(response_ids, skip_special_tokens=True)
            
            # EOS token'dan sonrasÄ±nÄ± temizle
            if self.lora_tokenizer.eos_token:
                generated_response = generated_response.split(self.lora_tokenizer.eos_token)[0].strip()
            
            generated_text = generated_response
            
            # Post-processing: "assistant:" ve "user:" Ã¶neklerini temizle
            assistant_match = re.search(r'assistant\s*:\s*', generated_text, flags=re.IGNORECASE)
            if assistant_match:
                generated_text = generated_text[assistant_match.end():].strip()
            
            user_match = re.search(r'user\s*:\s*', generated_text, flags=re.IGNORECASE)
            if user_match:
                generated_text = generated_text[user_match.end():].strip()
            
            # Prompt iÃ§eriyorsa temizle
            if prompt.strip() in generated_text:
                generated_text = generated_text.replace(prompt.strip(), "", 1).strip()
            
            # VirgÃ¼lle baÅŸlayan metinleri temizle
            if generated_text.startswith(','):
                parts = generated_text.split(',', 1)
                if len(parts) > 1:
                    generated_text = parts[1].strip()
                else:
                    generated_text = generated_text.lstrip(',').strip()
            
            # Prompt'un ilk birkaÃ§ kelimesini kontrol et ve varsa Ã§Ä±kar
            prompt_words = prompt.strip().split()[:5]
            if len(prompt_words) >= 3:
                prompt_prefix = ' '.join(prompt_words)
                if generated_text.startswith(prompt_prefix):
                    generated_text = generated_text[len(prompt_prefix):].strip()
            
            # Emoji sayÄ±sÄ±nÄ± sÄ±nÄ±rla (en fazla 1 emoji)
            generated_text = self._limit_emoji_count(generated_text, max_emojis=1)
            
            return generated_text
            
        except Exception as e:
            print(f"[ERROR] LoRA metin Ã¼retme hatasÄ±: {e}")
            return ""
    
    def _limit_emoji_count(self, text: str, max_emojis: int = 1) -> str:
        """Metindeki emoji sayÄ±sÄ±nÄ± sÄ±nÄ±rlar - dataset'e uygun"""
        import re
        
        # Unicode emoji pattern'i
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
            "\U0001FA00-\U0001FAFF"  # Chess Symbols
            "\U00002600-\U000026FF"  # Miscellaneous Symbols
            "\U00002700-\U000027BF"  # Dingbats
            "]+",
            flags=re.UNICODE
        )
        
        # TÃ¼m emoji bloklarÄ±nÄ± bul
        emoji_blocks = []
        i = 0
        while i < len(text):
            if emoji_pattern.match(text[i]):
                start = i
                end = i + 1
                while end < len(text):
                    if text[end] == '\u200d' and end + 1 < len(text):
                        if emoji_pattern.match(text[end + 1]):
                            end += 2
                        else:
                            break
                    elif emoji_pattern.match(text[end]):
                        end += 1
                    else:
                        break
                emoji_blocks.append((start, end))
                i = end
            else:
                i += 1
        
        # Emoji sayÄ±sÄ± max_emojis'den fazlaysa sadece ilk max_emojis kadarÄ±nÄ± tut
        if len(emoji_blocks) > max_emojis:
            parts = []
            last_end = 0
            
            for idx, (start, end) in enumerate(emoji_blocks[:max_emojis]):
                if last_end < start:
                    parts.append(text[last_end:start])
                last_end = end
            
            if last_end < len(text):
                remaining = text[last_end:]
                remaining = emoji_pattern.sub('', remaining)
                remaining = remaining.replace('\u200d', '')
                parts.append(remaining)
            
            kept_emojis = ''.join(text[start:end] for start, end in emoji_blocks[:max_emojis])
            
            result = ''.join(parts).strip()
            if kept_emojis:
                result = (result + ' ' + kept_emojis).strip() if result else kept_emojis
            
            text = result
        
        # Fazla boÅŸluklarÄ± temizle
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def chat(self, user_message: str) -> Dict[str, Any]:
        """Ana sohbet fonksiyonu - LoRA modelinden cevap, sonra LLM'den duygu"""
        # GÃ¼venlik kontrolleri
        if not user_message:
            return {"response": "Mesaj boÅŸ olamaz"}
        
        if not self._validate_emotion_message_length(user_message):
            return {"response": f"Mesaj Ã§ok uzun. Maksimum {MAX_EMOTION_MESSAGE_LENGTH} karakter olabilir."}
        
        user_message = self._sanitize_emotion_input(user_message)
        if user_message == "[GÃ¼venlik nedeniyle mesaj filtrelendi]":
            return {"response": "GÃ¼venlik nedeniyle mesaj filtrelendi"}
        
        self.stats["requests"] += 1
        self.stats["last_request_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # ADIM 1: LoRA modelinden cevap al (sadece kullanÄ±cÄ± mesajÄ± ile)
        print("[EMOTION] LoRA modelinden cevap alÄ±nÄ±yor...")
        
        # LoRA modeli Ã¶nceden yÃ¼klenmiÅŸ olmalÄ±, kontrol et
        if not self._lora_loaded:
            if self._lora_loading:
                print("[EMOTION] LoRA modeli hala yÃ¼kleniyor, bekleniyor...")
                import time
                for _ in range(60):
                    if self._lora_loaded:
                        break
                    time.sleep(1)
            if not self._lora_loaded:
                print("[EMOTION] LoRA modeli yÃ¼klenmedi, ÅŸimdi yÃ¼kleniyor...")
                self._load_lora_model()
        
        if not TRANSFORMERS_AVAILABLE or self.lora_model is None:
            return {"response": "LoRA model yÃ¼klenemedi. LÃ¼tfen transformers ve peft kÃ¼tÃ¼phanelerini yÃ¼kleyin."}
        
        try:
            import torch
        except ImportError:
            return {"response": "LoRA model yÃ¼klenemedi. LÃ¼tfen torch kÃ¼tÃ¼phanesini yÃ¼kleyin."}
        
        # LoRA'ya sadece kullanÄ±cÄ±nÄ±n mesajÄ±nÄ± gÃ¶nder
        lora_response = self._generate_with_lora(user_message, max_new_tokens=40)
        
        if not lora_response:
            return {"response": "LoRA modelinden cevap alÄ±namadÄ±."}
        
        print(f"[EMOTION] LoRA cevabÄ±: {lora_response[:100]}...")
        
        # ADIM 2: LLM'den duygu analizi (kullanÄ±cÄ± mesajÄ± + LoRA cevabÄ±)
        print("[EMOTION] LLM'den duygu analizi yapÄ±lÄ±yor...")
        
        emotion_prompt = f"""GÃ¶rev: Verilen kullanÄ±cÄ± mesajÄ±nÄ± ve asistan cevabÄ±nÄ± analiz et ve sadece 1 duygu belirle.

KullanÄ±cÄ± mesajÄ±: "{user_message}"
Asistan cevabÄ±: "{lora_response}"

Sadece aÅŸaÄŸÄ±daki JSON formatÄ±nÄ± dÃ¶ndÃ¼r, baÅŸka hiÃ§bir ÅŸey yazma:

{{"ruh_hali": "Mutlu"}}

SeÃ§ilebilecek ruh halleri (sadece bu listeden seÃ§):
- Mutlu
- ÃœzgÃ¼n
- Ã–fkeli
- ÅžaÅŸkÄ±n
- UtanmÄ±ÅŸ
- EndiÅŸeli
- GÃ¼lÃ¼mseyen
- FlÃ¶rtÃ¶z
- SorgulayÄ±cÄ±
- Yorgun

Ã–NEMLÄ°: Sadece JSON formatÄ±nda cevap ver. Metin ekleme, aÃ§Ä±klama yapma."""
        
        messages_payload: list[Dict[str, Any]] = [
            {"role": "system", "content": "Sen bir duygu analiz asistanÄ±sÄ±n. Verilen metni analiz edip sadece JSON formatÄ±nda duygu dÃ¶ndÃ¼rÃ¼rsÃ¼n. BaÅŸka hiÃ§bir ÅŸey yazmazsÄ±n."},
            {"role": "user", "content": emotion_prompt}
        ]
        
        # LLM'den duygu analizi al
        try:
            if self.use_gemini:
                import google.generativeai as genai
                model = genai.GenerativeModel('gemini-2.5-flash')
                prompt_text = self._convert_messages_to_prompt(messages_payload)
                response = model.generate_content(prompt_text)
                emotion_content = response.text
                print(f"[EMOTION] Gemini duygu cevabÄ±: {emotion_content}")
            else:
                completion = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=messages_payload,
                    temperature=0.2,
                )
                emotion_content = completion.choices[0].message.content or ""
        except Exception as e:
            print(f"[ERROR] LLM duygu analizi hatasÄ±: {e}")
            emotion_content = json.dumps({
                "ruh_hali": random.choice(["Mutlu", "ÃœzgÃ¼n", "ÅžaÅŸkÄ±n"])
            })
        
        # JSON Ã§Ä±kar
        def extract_json_object(text: str) -> Dict[str, Any] | None:
            t = text.replace("```json", "").replace("```", "").strip()
            start = t.find('{')
            if start == -1:
                return None
            depth = 0
            in_string = False
            escape = False
            end_index = -1
            for i in range(start, len(t)):
                ch = t[i]
                if in_string:
                    if escape:
                        escape = False
                    elif ch == '\\':
                        escape = True
                    elif ch == '"':
                        in_string = False
                else:
                    if ch == '"':
                        in_string = True
                    elif ch == '{':
                        depth += 1
                    elif ch == '}':
                        depth -= 1
                        if depth == 0:
                            end_index = i
                            break
            if end_index == -1:
                return None
            candidate = t[start:end_index + 1]
            try:
                return json.loads(candidate)
            except Exception:
                return None
        
        emotion_data = extract_json_object(emotion_content)
        
        # Fallback: JSON parse edilemezse rastgele duygu
        if not emotion_data:
            print("[WARNING] LLM'den JSON parse edilemedi, rastgele duygu seÃ§iliyor")
            emotion_data = {
                "ruh_hali": random.choice(["Mutlu", "ÃœzgÃ¼n", "ÅžaÅŸkÄ±n"])
            }
        
        # Duygu kaydÄ±nÄ± zaman damgasÄ±yla ekle
        mood_raw = str(emotion_data.get("ruh_hali", ""))
        if mood_raw.strip() in self.emotion_counts:
            self.emotion_counts[mood_raw.strip()] += 1
            # Zaman damgalÄ± kayÄ±t ekle
            self._append_mood_record(mood_raw.strip())
            print(f"[EMOTION] Duygu kaydedildi: {mood_raw.strip()}")
        else:
            print(f"[EMOTION] Duygu kaydedilemedi: '{mood_raw.strip()}' allowed_moods listesinde yok")
        
        # Emoji seÃ§im: mood_emojis.json'dan duyguya gÃ¶re rastgele
        def normalize_mood(name: str) -> str:
            """Duygu ismini mood_emojis.json'daki anahtarlara normalize eder"""
            n = name.strip()
            n_lower = n.lower()
            
            # Direkt JSON anahtarlarÄ±nÄ± kontrol et
            json_keys = list(MOOD_EMOJIS.keys())
            for key in json_keys:
                if key.lower() == n_lower:
                    return key
            
            # YazÄ±m hatalarÄ±nÄ± dÃ¼zelt
            mapping = {
                "gÃ¼llÃ¼mseyen": "GÃ¼lÃ¼mseyen",
                "gÃ¼lÃ¼mseyen": "GÃ¼lÃ¼mseyen",
                "gullumseyen": "GÃ¼lÃ¼mseyen",
                "gulumsyen": "GÃ¼lÃ¼mseyen",
                "utangaÃ§": "UtanmÄ±ÅŸ",
                "utanmÄ±ÅŸ": "UtanmÄ±ÅŸ",
                "utanmis": "UtanmÄ±ÅŸ",
                "utangac": "UtanmÄ±ÅŸ",
                "mutlu": "Mutlu",
                "Ã¼zgÃ¼n": "ÃœzgÃ¼n",
                "uzgun": "ÃœzgÃ¼n",
                "Ã¶fkeli": "Ã–fkeli",
                "ofkeli": "Ã–fkeli",
                "ÅŸaÅŸkÄ±n": "ÅžaÅŸkÄ±n",
                "saskin": "ÅžaÅŸkÄ±n",
                "endiÅŸeli": "EndiÅŸeli",
                "endiseli": "EndiÅŸeli",
                "flÃ¶rtÃ¶z": "FlÃ¶rtÃ¶z",
                "flortoz": "FlÃ¶rtÃ¶z",
                "flÃ¶rtoz": "FlÃ¶rtÃ¶z",
                "flortÃ¶z": "FlÃ¶rtÃ¶z",
                "sorgulayÄ±cÄ±": "SorgulayÄ±cÄ±",
                "sorgulayici": "SorgulayÄ±cÄ±",
                "yorgun": "Yorgun",
            }
            
            normalized = mapping.get(n_lower)
            if normalized and normalized in json_keys:
                return normalized
            
            return n
        
        def pick_emoji(mood: str) -> Optional[str]:
            """mood_emojis.json'dan duyguya gÃ¶re rastgele emoji seÃ§er"""
            key = normalize_mood(mood)
            print(f"[EMOTION] Duygu normalize edildi: '{mood}' -> '{key}'")
            
            # JSON'daki anahtarlarÄ± direkt kontrol et
            options = MOOD_EMOJIS.get(key)
            if options:
                try:
                    selected_emoji = random.choice(options)
                    print(f"[EMOTION] Emoji seÃ§ildi: {selected_emoji} (duygu: {key}, seÃ§enekler: {len(options)})")
                    return selected_emoji
                except Exception as e:
                    print(f"[EMOTION] Emoji seÃ§im hatasÄ±: {e}")
                    return None
            
            # Fallback: fuzzy matching
            print(f"[EMOTION] Direkt eÅŸleÅŸme bulunamadÄ±, fuzzy matching deneniyor...")
            for json_key in MOOD_EMOJIS.keys():
                if json_key.lower() in key.lower() or key.lower() in json_key.lower():
                    options = MOOD_EMOJIS.get(json_key)
                    if options:
                        try:
                            selected_emoji = random.choice(options)
                            print(f"[EMOTION] Emoji seÃ§ildi (fuzzy): {selected_emoji} (duygu: {key} -> {json_key})")
                            return selected_emoji
                        except Exception as e:
                            print(f"[EMOTION] Fuzzy emoji seÃ§im hatasÄ±: {e}")
                            continue
            
            print(f"[EMOTION] Emoji bulunamadÄ±: {key}")
            return None
        
        emoji = pick_emoji(mood_raw)
        if emoji:
            print(f"[EMOTION] Final emoji: {emoji}")
        else:
            print(f"[EMOTION] WARNING: Emoji None dÃ¶ndÃ¼! Duygu: {mood_raw}")
            # Fallback: eÄŸer emoji bulunamazsa varsayÄ±lan emoji kullan
            emoji = 'ðŸ™‚'
            print(f"[EMOTION] Fallback emoji kullanÄ±lÄ±yor: {emoji}")
        
        # KonuÅŸma geÃ§miÅŸini kaydet
        self._append_chat_history(user_message, lora_response)
        
        # Response format: Frontend'in beklediÄŸi format
        return {
            "response": lora_response,  # LoRA cevabÄ±
            "emoji": emoji,  # Tek emoji (mutlaka bir deÄŸer olmalÄ±)
            "mood": mood_raw,  # Duygu
            "stats": self.stats,  # Ä°statistikler
        }
